{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Find Header_Tags Of Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_header_tag(url):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    \n",
    "#Now, we will send get request to the webpage server to get the source code of the page\n",
    "#which is to be scraped by using the request library\n",
    "#page = requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    header_tags = []\n",
    "    for i in soup.find_all('span', class_='mw-headline'):\n",
    "        header_tags.append(i.get_text().replace('\\n', ''))\n",
    "    htags = pd.DataFrame({})\n",
    "    htags['Header Tags'] = header_tags\n",
    "    return htags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Header Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From today's featured article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did you know ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On this day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Today's featured picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Other areas of Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wikipedia's sister projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wikipedia languages</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Header Tags\n",
       "0  From today's featured article\n",
       "1               Did you know ...\n",
       "2                    In the news\n",
       "3                    On this day\n",
       "4       Today's featured picture\n",
       "5       Other areas of Wikipedia\n",
       "6    Wikipedia's sister projects\n",
       "7            Wikipedia languages"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_header_tag('https://en.wikipedia.org/wiki/Main_Page')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Function to Scrap IMDB site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top100Movies_IMDB(url):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    \n",
    "#Getting Data Fromm Webpage, for this we studied website and came to know that..\n",
    "#advanced search will help to get the below url\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    Movie_Names, Year_Of_Release, Ratings = [],[],[]\n",
    "    \n",
    "    for i in soup.find_all('h3', class_ = 'lister-item-header'):\n",
    "            Movie_Names.append(i.a.text)\n",
    "    for i in soup.find_all('span', class_ = 'lister-item-year text-muted unbold'):\n",
    "        Year_Of_Release.append(''.join(filter(lambda j: j.isdigit(), i.text.replace('\\n','').replace(' ', ''))))\n",
    "    Year_Of_Release = [sub.replace('(', '').replace(\")\",\"\").replace('-','') for sub in Year_Of_Release]\n",
    "    for i in soup.find_all('div', class_ = 'inline-block ratings-imdb-rating'):\n",
    "        Ratings.append(i.strong.text)\n",
    "\n",
    "#Values into Data Frame:-\n",
    "    Movie_lst = pd.DataFrame({})\n",
    "    Movie_lst['Movie Name'] = pd.Series(Movie_Names)\n",
    "    Movie_lst['Movie Release Year'] = pd.Series(Year_Of_Release)\n",
    "    Movie_lst['Movie Rating'] = pd.Series(Ratings)\n",
    "    Movie_lst = Movie_lst.sort_values(by=['Movie Rating'], ascending = False)\n",
    "    Movie_lst = Movie_lst.head(100)\n",
    "\n",
    "#Data Frame to CSV export(zip file):-\n",
    "    if (\"countries=in\" in url):\n",
    "        compression_opts = dict(method='zip', archive_name='TopMovies_Indian.csv')\n",
    "        return Movie_lst.to_csv('TopMovies_Indian.zip', index=False, compression=compression_opts)\n",
    "    else:\n",
    "        compression_opts = dict(method='zip', archive_name='TopMovies.csv')\n",
    "        return Movie_lst.to_csv('TopMovies.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find Top 100 Movies From IMDB site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top100Movies(url):\n",
    "    url = url+'/search/title/?title_type=feature&sort=num_votes,desc&count=250'\n",
    "    return top100Movies_IMDB(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_top100Movies('https://www.imdb.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find Top 100 Indian Movies in IMDB site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top100Movies_Indian(url):\n",
    "    url = url+'/search/title/?title_type=feature&countries=in&adult=include&sort=num_votes,desc&count=250'\n",
    "    return top100Movies_IMDB(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_top100Movies_Indian('https://www.imdb.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Book name, author name, genre and book review of any 5 books from ‘www.bookpage.com’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_reviews(url):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    Book_Names, Author_Names, Genre, Book_Reviews, review_links = [],[],[],[],[]\n",
    "#On study, for this site there is seperate webpage to get reviews to simplify that....\n",
    "#we create another function to get review data onto single list\n",
    "    review_links = get_review_links(url)\n",
    "    \n",
    "    for i in review_links:\n",
    "        page_review = requests.get(url+i)\n",
    "        soup = BeautifulSoup(page_review.content, 'html.parser')\n",
    "        Name,Author,Genres,review = [],[],[],[]\n",
    "        \n",
    "        for j in soup.find_all('h1', class_ = 'italic'):\n",
    "            Book_Names.append(j.get_text().replace('\\n',''))\n",
    "        for j in soup.find_all('h4', class_ = 'sans'):\n",
    "            Author_Names.append(j.text.replace('\\n',''))\n",
    "        for j in soup.find_all('p', class_ = 'genre-links'):\n",
    "            Genre.append(j.a.text)\n",
    "        review_temp = []\n",
    "        for j in soup.find_all('div', class_ = 'article-body'):\n",
    "            children = j.findChildren('p', recursive=False)\n",
    "            for child in children:\n",
    "                review_temp.append(child.text.replace('\\n',''))\n",
    "            Book_Reviews.append(review_temp)\n",
    "            \n",
    "#Values into Data Frame:-\n",
    "    Book_lst = pd.DataFrame({})\n",
    "    Book_lst['Book Name'] = pd.Series(Book_Names) #To handle NaN values, we converted into Series\n",
    "    Book_lst['Author Name'] = pd.Series(Author_Names)\n",
    "    Book_lst['Genre'] = pd.Series(Genre)\n",
    "    Book_lst['Book Review'] = pd.Series(Book_Reviews)\n",
    "    Book_lst = Book_lst.head()\n",
    "    Book_lst\n",
    "#Data Frame to CSV export(zip file):-\n",
    "    compression_opts = dict(method='zip', archive_name='Book_Reviews.csv')\n",
    "    return Book_lst.to_csv('Book_Reviews.zip', index=False, compression=compression_opts)  \n",
    "\n",
    "\n",
    "#function to get review links of different pages\n",
    "def get_review_links(url):\n",
    "    url = url+'/reviews'\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    td = soup.find_all('div', class_='read-full')\n",
    "    links =[]\n",
    "    for i in td:\n",
    "        children = i.findChildren(\"a\" , recursive=True)\n",
    "        for child in children:\n",
    "            links.append(child.get('href'))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_reviews('https://bookpage.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are Common Function to scrap ICC Cricket Site as per requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This Function is to get top teams from both men and women cricket team from ‘www.icc-cricket.com’\n",
    "#depends upon url passing through its parent funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This Function is to get top teams from both men and women cricket team from ‘www.icc-cricket.com’\n",
    "#depends upon url passing through its parent funcion\n",
    "def Top10_teams(url):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import pandas as pd  \n",
    "    Team_Position, Team_Name, No_of_Matches, Points, Rating = [],[],[],[],[]    \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    position = soup.find_all('td', {'class': ['rankings-block__banner--pos','table-body__cell table-body__cell--position u-text-right']})\n",
    "    t_name = soup.find_all('span', class_='u-hide-phablet')\n",
    "    match = soup.find_all('td',{'class':['table-body__cell u-center-text', 'rankings-block__banner--matches','rankings-block__banner--points']})\n",
    "    rate = soup.find_all('td',{'class':['rankings-block__banner--rating u-text-right', 'table-body__cell u-text-right rating']})\n",
    "    \n",
    "    for i in position:\n",
    "        Team_Position.append(i.text.replace('\\n',''))\n",
    "    for i in t_name:\n",
    "        Team_Name.append(i.text.replace('\\n',''))\n",
    "    for i, j in enumerate(match):\n",
    "        if i % 2 ==0:\n",
    "            No_of_Matches.append(j.text.replace('\\n',''))\n",
    "        else:\n",
    "            Points.append(j.text.replace('\\n',''))\n",
    "    for i in rate:\n",
    "        Rating.append(i.text.replace('\\n','').replace(' ', ''))\n",
    "\n",
    "    #Values into Data Frame:-    \n",
    "    Team_lst = pd.DataFrame({})\n",
    "    Team_lst['Team Position'] = pd.Series(Team_Position) #To handle NaN values(If there is any), we converted into Series\n",
    "    Team_lst['Team Name'] = pd.Series(Team_Name)\n",
    "    Team_lst['No.Of Matches'] = pd.Series(No_of_Matches)\n",
    "    Team_lst['Points'] = pd.Series(Points)\n",
    "    Team_lst['Rating'] = pd.Series(Rating)\n",
    "    Team_lst = Team_lst.head(10)\n",
    "    Team_lst\n",
    "\n",
    "    #Data Frame to CSV export(zip file):-\n",
    "    if (\"womens\" in url):\n",
    "        compression_opts = dict(method='zip', archive_name='Top10_ODI_Womens_Team.csv')\n",
    "        return Team_lst.to_csv('Top10_ODI_Womens_Team.zip', index=False, compression=compression_opts)\n",
    "    elif (\"mens\" in url):\n",
    "        compression_opts = dict(method='zip', archive_name='Top10_ODI_Mens_Team.csv')\n",
    "        return Team_lst.to_csv('Top10_ODI_Mens_Team.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This Function is to get top players from both batting, bowling, men and womens team from ‘www.icc-cricket.com’\n",
    "#depends upon url passing through its parent funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_Top10_player(url):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    Player_Position, Player_Name, Team, Rating, Career_best_rating = [],[],[],[],[]\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    position = soup.find_all('td', {'class': ['rankings-block__position','table-body__cell table-body__cell--position u-text-right']})\n",
    "    p_name = soup.find_all('div', class_='rankings-block__banner--name-large')\n",
    "    p_name1 = soup.find_all('td', class_='table-body__cell rankings-table__name name')\n",
    "    match = soup.find_all('span',class_='table-body__logo-text')\n",
    "    match1 = soup.find_all('div', class_= 'rankings-block__banner--nationality')\n",
    "    rate = soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "    rate1 = soup.find_all('td',class_='table-body__cell rating')\n",
    "    best = soup.find_all('span',class_='rankings-block__career-best-text')\n",
    "    best1 = soup.find_all('td',class_='table-body__cell u-text-right u-hide-phablet')\n",
    " \n",
    "    for i in position:\n",
    "        Player_Position.append(''.join(filter(lambda j: j.isdigit(), i.text.replace('\\n','').replace(' ', ''))))\n",
    "    for i in p_name:\n",
    "        Player_Name.append(i.text.replace('\\n',''))\n",
    "    for i in p_name1:\n",
    "        Player_Name.append(i.a.text.replace('\\n',''))\n",
    "    for i in (match1+match):\n",
    "        Team.append(i.text.replace('\\n',''))\n",
    "    for i in (rate+rate1):\n",
    "        Rating.append(i.text.replace('\\n',''))\n",
    "    for i in (best+best1):\n",
    "        Career_best_rating.append(i.text.replace('\\n','').replace(' ', ''))\n",
    "        \n",
    "#Values into Data Frame:-    \n",
    "    player_lst = pd.DataFrame({})\n",
    "    player_lst['Position'] = pd.Series(Player_Position) #To handle NaN values(If there is any), we converted into Series\n",
    "    player_lst['Player Name'] = pd.Series(Player_Name)\n",
    "    player_lst['Player Team'] = pd.Series(Team)\n",
    "    player_lst['Rating'] = pd.Series(Rating)\n",
    "    player_lst['Career best rating'] = pd.Series(Career_best_rating)\n",
    "    player_lst = player_lst.head(10)\n",
    "    player_lst\n",
    "    \n",
    "#Data Frame to CSV export(zip file):-\n",
    "    if ((\"batting\" in url) and(\"/mens/\" in url)):\n",
    "        compression_opts = dict(method='zip', archive_name='Top10_ODI_Mens_Batting.csv')\n",
    "        return player_lst.to_csv('Top10_ODI_Mens_Batting.zip', index=False, compression=compression_opts)\n",
    "    elif ((\"batting\" in url) and(\"/womens/\" in url)):\n",
    "        compression_opts = dict(method='zip', archive_name='Top10_ODI_womens_Batting.csv')\n",
    "        return player_lst.to_csv('Top10_ODI_womens_Batting.zip', index=False, compression=compression_opts) \n",
    "    elif ((\"bowling\" in url) and(\"/mens/\" in url)):\n",
    "        compression_opts = dict(method='zip', archive_name='Top10_ODI_Mens_Bowling.csv')\n",
    "        return player_lst.to_csv('Top10_ODI_Mens_Bowling.zip', index=False, compression=compression_opts)\n",
    "    elif ((\"bowling\" in url)and(\"/womens/\" in url)):\n",
    "        compression_opts = dict(method='zip', archive_name='Top10_ODI_womens_Bowling.csv')\n",
    "        return player_lst.to_csv('Top10_ODI_womens_Bowling.zip', index=False, compression=compression_opts)\n",
    "    elif ((\"/womens/\" in url)and(\"all-rounder\" in url)):\n",
    "        compression_opts = dict(method='zip', archive_name='Top10_ODI_womens_allrounder.csv')\n",
    "        return player_lst.to_csv('Top10_ODI_womens_allrounder.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Python program to scrape cricket rankings from ‘www.icc-cricket.com’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_Top10_ODI_MensTeam(url):\n",
    "    url = url+'/rankings/mens/team-rankings/odi'\n",
    "    return Top10_teams(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_Top10_ODI_MensTeam('https://www.icc-cricket.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Top 10 ODI Batsmen in men along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_Top10_ODI_MenBatting(url):\n",
    "    url = url+'/rankings/mens/player-rankings/odi/batting'\n",
    "    return find_Top10_player(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_Top10_ODI_MenBatting('https://www.icc-cricket.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) Top 10 ODI bowlers in men along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_Top10_ODI_MenBowling(url):\n",
    "    url = url+'/rankings/mens/player-rankings/odi/bowling'\n",
    "    return find_Top10_player(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_Top10_ODI_MenBowling('https://www.icc-cricket.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Python program to scrape cricket rankings from ‘www.icc-cricket.com’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_Top10_ODI_WomensTeam(url):\n",
    "    url = url+'/rankings/womens/team-rankings/odi'\n",
    "    return Top10_teams(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_Top10_ODI_WomensTeam('https://www.icc-cricket.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Top 10 women’s ODI players along with the records of their team and rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_Top10_WomensODI_player(url):\n",
    "    url = url+'/rankings/womens/player-rankings/odi/batting'\n",
    "    return find_Top10_player(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_Top10_WomensODI_player('https://www.icc-cricket.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_Top10_WomensODI_AllRounder(url):\n",
    "    url = url+'/rankings/womens/player-rankings/odi/all-rounder'\n",
    "    return find_Top10_player(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_Top10_WomensODI_AllRounder('https://www.icc-cricket.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Python program to scrape details of all the mobile phones under Rs. 20,000 listed on Amazon.in. The scraped data should include Product Name, Price, Image URL and Average Rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Amazon_mbl_20k(url):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import pandas as pd  \n",
    "    import time\n",
    "    Product_Name, Price, Average_Rating, Image_Url= [],[],[],[] \n",
    "    url = url+'/s?i=electronics&bbn=1389401031&rh=n%3A976419031%2Cn%3A1389401031%2Cn%3A1389432031%2Cp_36%3A-2000000&dc&qid=1610187018&rnid=1389401031&ref=sr_nr_n_3'\n",
    "\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "    name = soup.find_all('span', class_='a-size-base-plus a-color-base a-text-normal')\n",
    "    rating = soup.find_all('a', class_='a-popover-trigger a-declarative')\n",
    "    price = soup.find_all('span', {'class':['a-price-whole','a-offscreen']})\n",
    "    imageurl = soup.find_all('img', class_='s-image')\n",
    "\n",
    "\n",
    "    for i in name:\n",
    "        Product_Name.append(i.text.replace('\\n',''))\n",
    "    for j in rating:\n",
    "        Average_Rating.append(j.span.text.replace('\\n',''))\n",
    "    for k in price:\n",
    "        Price.append(''.join(filter(lambda j: j.isdigit(), k.text.replace('\\n','').replace(' ', ''))))\n",
    "    for l in imageurl:\n",
    "        Image_Url.append(l.attrs['src'])\n",
    "\n",
    "\n",
    "\n",
    "    amazon_lst = pd.DataFrame({})\n",
    "    amazon_lst['Mobile Name'] = pd.Series(Product_Name) #To handle NaN values(If there is any), we converted into Series\n",
    "    amazon_lst['Mobile Price'] = pd.Series(Price)\n",
    "    amazon_lst['Average Rating'] = pd.Series(Average_Rating)\n",
    "    amazon_lst['Image URL'] = pd.Series(Image_Url)\n",
    "    #amazon_lst = amazon_lst.sort_values(by=['Mobile Price'], ascending =False)\n",
    "\n",
    "    compression_opts = dict(method='zip', archive_name='Amazon_Mbls_under_20k.csv')\n",
    "    return amazon_lst.to_csv('Amazon_Mbls_under_20k.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "Amazon_mbl_20k('https://www.amazon.in')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. A python program to extract information about the local weather from the National Weather Service website of USA, https://www.weather.gov/ for the city, San Francisco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to extract data about 7 day\n",
    "extended forecast display for the city. The data should include period, short description, temperature and\n",
    "description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_Forecast(url):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import pandas as pd  \n",
    "\n",
    "    Period, Short_Description, Temperature, Description= [],[],[],[]\n",
    "    url = url+'/MapClick.php?lat=37.777120000000025&lon=-122.41963999999996#.X_pvTugzbIU'\n",
    "\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    for i in soup.find_all('div', class_='col-sm-2 forecast-label'):\n",
    "        Period.append(i.text)\n",
    "\n",
    "    for i in soup.find_all('div', class_='col-sm-10 forecast-text'):\n",
    "        desc = i.text.replace('\\n','')\n",
    "        temp = (''.join(filter(lambda j: j.isdigit(), desc)))\n",
    "        Temperature.append(temp[:2])\n",
    "        Description.append(desc)\n",
    "        desc_split = desc.split(',')\n",
    "        if len(desc_split)>2:\n",
    "            Short_Description.append((' '.join(desc_split[0:2])))\n",
    "        else:\n",
    "            Short_Description.append(desc_split[0])\n",
    "\n",
    "    weather_df = pd.DataFrame({})\n",
    "    weather_df['Period'] = pd.Series(Period) #To handle NaN values(If there is any), we converted into Series\n",
    "    weather_df['Short Description'] = pd.Series(Short_Description)\n",
    "    weather_df['Temperature'] = pd.Series(Temperature)\n",
    "    weather_df['Description'] = pd.Series(Description)\n",
    "\n",
    "\n",
    "    compression_opts = dict(method='zip', archive_name='weather.csv')\n",
    "    return weather_df.to_csv('weather_report.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_Forecast('https://forecast.weather.gov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. python program to scrape ‘software developer’ job listings from ‘Monster.com’. It should include all the jobs listed for the next 5 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<HTML><HEAD>\\n<TITLE>Access Denied</TITLE>\\n</HEAD><BODY>\\n<H1>Access Denied</H1>\\n \\nYou don\\'t have permission to access \"http&#58;&#47;&#47;www&#46;monsterindia&#46;com&#47;srp&#47;results&#63;\" on this server.<P>\\nReference&#32;&#35;18&#46;8dc72c31&#46;1610272896&#46;d29a3a9d\\n</BODY>\\n</HTML>\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd \n",
    "\n",
    "url = 'https://www.monsterindia.com/srp/results?query=Software%20Developer&searchId=16f7d6f8-55fe-4a1a-9d4f-11d0934edaa7'\n",
    "page = requests.get(url)\n",
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the Monster.com is not allowing Web Scrapping, We are doing same exercise on Naukri.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naukri.com was not possible with BeautifulSoup, So we used Selenium for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting selenium\n",
      "\n",
      "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.25.9)\n",
      "Installing collected packages: selenium\n",
      "Successfully installed selenium-3.141.0\n"
     ]
    }
   ],
   "source": [
    "#pip install selenium #Please Don't Run Already Installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. python program to scrape ‘data scientist’ job listings for location ‘New Delhi’ from ‘Monster.com’. It should include all the jobs listed for the next 5 pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the Monster.com is not allowing Web Scrapping, I am doing same exercise on Naukri.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naukri.com was not possible with BeautifulSoup, So we used Selenium for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common Function to get required data for question 9 and 10 in webscrapping assignment\n",
    "def Job_report(url):\n",
    "    import selenium\n",
    "    import pandas as pd\n",
    "    from selenium import webdriver\n",
    "    import time\n",
    "\n",
    "    driver = webdriver.Chrome(r'C:\\Users\\udayakiran\\Desktop\\chromedriver.exe') # system local Chromedriver location path\n",
    "    driver.get(url)\n",
    "    #time.sleep(15) #15sec sleep mode Time to open browser and load required url site\n",
    "    \n",
    "    Job_title, Company_Name= [],[]\n",
    "    if(\"data-scientist\" in url) and ('new%20delhi' in url):\n",
    "        City=[]\n",
    "        \n",
    "\n",
    "    for i in range(0,5):#range For First five pages loop\n",
    "        for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\"):\n",
    "            Job_title.append(i.text)\n",
    "        for j in driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\"):\n",
    "            Company_Name.append(j.text)\n",
    "            if(\"data-scientist\" in url) and ('new%20delhi' in url):\n",
    "                for k in driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\"):\n",
    "                    City.append(k.text)\n",
    "        driver.find_element_by_xpath(\"//a[@class='fright fs14 btn-secondary br2']\").click()\n",
    "        time.sleep(3)\n",
    "\n",
    "    job_lst = pd.DataFrame({})\n",
    "    job_lst['Job Title'] = pd.Series(Job_title)\n",
    "    job_lst['Company Name'] = pd.Series(Company_Name)\n",
    "    if(\"data-scientist\" in url) and ('new%20delhi' in url):\n",
    "        job_lst['Location'] = pd.Series(City)\n",
    "    \n",
    "    if(\"data-scientist\" in url) and ('new%20delhi' in url):\n",
    "        compression_opts = dict(method='zip', archive_name='Job_report_of_DS_Delhi_Location.csv')\n",
    "        return job_lst.to_csv('Job_report_of_DS_Delhi_Location.zip', index=False, compression=compression_opts)\n",
    "    else:\n",
    "        compression_opts = dict(method='zip', archive_name='Job_Report_of_5pages.csv')\n",
    "        return job_lst.to_csv('Job_Report_of_5pages.zip', index=False, compression=compression_opts)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Job_report_of_5pages(url):\n",
    "    url = url+\"/software-developer-jobs?k=software%20developer\"\n",
    "    return Job_report(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Job_report_of_DS_Delhi_Location(url):\n",
    "    url = url+\"/data-scientist-jobs-in-new-delhi?k=data%20scientist&l=new%20delhi\"\n",
    "    return Job_report(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_report_of_5pages('https://www.naukri.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_report_of_DS_Delhi_Location('https://www.naukri.com')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
