{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB SCRAPING ASSIGNMENT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_DataAnalyst_Posts_Bangalore(url):\n",
    "    import selenium\n",
    "    from selenium import webdriver\n",
    "    import time\n",
    "    import pandas as pd\n",
    "\n",
    "#1. first get the webpage https://www.naukri.com/\n",
    "\n",
    "    driver = webdriver.Chrome(r'C:\\Users\\udayakiran\\Desktop\\chromedriver.exe') # system local Chromedriver location path\n",
    "    driver.get(url) #accessing the website using driver\n",
    "    time.sleep(5)\n",
    "    \n",
    "#2. Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "\n",
    "    # Select the “Skill,Designations,Companies” field\n",
    "    Kw_box = driver.find_element_by_name('keyword')\n",
    "    #value = input('Please enter Job Title :\\n')#Enabling this line and below line will allow user to enter required Job title and proceed accordingly\n",
    "    #Kw_box.send_keys(value)#before enabling this line, please comment the below line\n",
    "    Kw_box.send_keys(\"Data Analyst\") #Entering data into Search key skills box\n",
    "\n",
    "    # Select “enter the location” field\n",
    "    loc_box = driver.find_element_by_name('location')\n",
    "    #value1 = input('Please enter Location you want:\\n') #Enabling this line and below line will allow user to enter required location and proceed accordingly\n",
    "    #loc_box.send_keys('value1') #before enabling this line, please comment the below line\n",
    "    loc_box.send_keys('Bangalore') #Entering data into Search location box\n",
    "\n",
    "#3. Then click the search button.\n",
    "    \n",
    "    # Find and click search button\n",
    "    search_button = driver.find_element_by_class_name('btn')\n",
    "    search_button.click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "#4. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "    Job_title,Company_Name,job_location,experience_required = [],[],[],[]\n",
    "    #Getting data for each field we required and send the aquired data into respective lists   \n",
    "    for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")[:10]:\n",
    "        Job_title.append(i.text)\n",
    "    for i in driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")[:10]:\n",
    "        Company_Name.append(i.text)\n",
    "    for i in driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")[:10]:\n",
    "        job_location.append(i.text)\n",
    "    for i in driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")[:10]:\n",
    "        experience_required.append(i.text)\n",
    "    driver.close()\n",
    "    \n",
    "    \n",
    "#5. Finally create a dataframe of the scraped data\n",
    "\n",
    "    #Creating Data Frame for Job details storing\n",
    "    job_lst = pd.DataFrame({})\n",
    "    job_lst['Job Title'] = pd.Series(Job_title)\n",
    "    job_lst['Company Name'] = pd.Series(Company_Name)\n",
    "    job_lst['Job Location'] = pd.Series(job_location)\n",
    "    job_lst['Experience_Required'] = pd.Series(experience_required)\n",
    "    #job_lst = job_lst[:10] #As per requirment we need only first 10 records from the result\n",
    "\n",
    "    #Exporting Data from data frame to CSV file and making ZIP folder\n",
    "    compression_opts = dict(method='zip', archive_name='Job_report_of_Data_Analyst_Bangalore.csv')\n",
    "    return job_lst.to_csv('Job_report_of_Data_Analyst_Bangalore.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_DataAnalyst_Posts_Bangalore(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_DataScientist_Bangalore(url):\n",
    "    import selenium\n",
    "    from selenium import webdriver\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "#1. first get the webpage https://www.naukri.com/\n",
    "\n",
    "    driver = webdriver.Chrome(r'C:\\Users\\udayakiran\\Desktop\\chromedriver.exe') # system local Chromedriver location path\n",
    "    driver.get(url) #accessing the website using driver\n",
    "    time.sleep(3)\n",
    "\n",
    "#2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "\n",
    "        # Select the “Skill,Designations,Companies” field\n",
    "    Kw_box = driver.find_element_by_name('keyword')\n",
    "    Kw_box.send_keys(\"Data Scientist\") #Entering data into Search key skills box\n",
    "\n",
    "        # Select “enter the location” field\n",
    "    loc_box = driver.find_element_by_name('location')\n",
    "    loc_box.send_keys('Bangalore') #Entering data into Search location box\n",
    "\n",
    "#3. Then click the search button.\n",
    "\n",
    "        # Find and click search button\n",
    "    search_button = driver.find_element_by_class_name('btn')\n",
    "    search_button.click()\n",
    "    time.sleep(3)\n",
    "\n",
    "#4. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    url_tags = soup.find_all('article', attrs = {'class': 'jobTuple bgWhite br4 mb-8'})\n",
    "    urls = [i.find('div').find_all('a')[0]['href'] for i in url_tags[0:10]]\n",
    "    urls\n",
    "    Job_title,Company_Name,job_location,Full_Job_Description = [],[],[],[]\n",
    "    for urll in urls:\n",
    "        driver.get(urll)\n",
    "\n",
    "        try:\n",
    "            Job_title.append((driver.find_element_by_class_name(\"jd-header-title\")).text.replace('\\n',''))\n",
    "        except NoSuchElementException as e:\n",
    "            try:\n",
    "                Job_title.append((driver.find_element_by_class_name(\"av-special-heading-tag \")).text.replace('\\n',''))\n",
    "            except NoSuchElementException as e:\n",
    "                Job_title.append((driver.find_element_by_xpath(\"//div[@class='cTitle f16 lh30']/h1[1]\").text.replace('\\n','')))\n",
    "        try:\n",
    "            Company_Name.append((driver.find_element_by_class_name(\"pad-rt-8\")).text.replace('\\n',''))\n",
    "        except NoSuchElementException as e:\n",
    "            try:          \n",
    "                Company_Name.append((driver.find_element_by_xpath(\"//div[@class='f14 lh18 alignJ']/p[1]\").text.replace('\\n','')))\n",
    "            except NoSuchElementException as e:\n",
    "                Company_Name.append((driver.find_element_by_xpath(\"//div[@class='f14 lh18 alignJ']\").text.replace('\\n','')))\n",
    "\n",
    "        try:\n",
    "            job_location.append((driver.find_element_by_class_name(\"location\")).text.replace('\\n',''))\n",
    "        except NoSuchElementException as e:\n",
    "            try:\n",
    "                job_location.append((driver.find_element_by_xpath(\"//span[@class='slide-meta-loc pull-left']\").text.replace('\\n','')))\n",
    "            except NoSuchElementException as e:\n",
    "                job_location.append((driver.find_element_by_xpath(\"//span[@class='fl disc-li']/a[1]\").text.replace('\\n','')))\n",
    "\n",
    "        try:\n",
    "            Full_Job_Description.append((driver.find_element_by_class_name(\"dang-inner-html\")).text.replace('\\n',''))\n",
    "        except NoSuchElementException as e:\n",
    "            try:           \n",
    "                Full_Job_Description.append((driver.find_element_by_xpath(\"//div[@class='clearboth description']\").text.replace('\\n','')))\n",
    "            except NoSuchElementException as e:\n",
    "                Full_Job_Description.append((driver.find_element_by_xpath(\"//div[@class='f14 lh18 alignJ disc-li']\").text.replace('\\n','')))\n",
    "    driver.close()\n",
    "#5. Finally create a dataframe of the scraped data\n",
    "\n",
    "        #Creating Data Frame for Job details storing\n",
    "    job_lst = pd.DataFrame({})\n",
    "    job_lst['Job Title'] = pd.Series(Job_title)\n",
    "    job_lst['Company Name'] = pd.Series(Company_Name)\n",
    "    job_lst['Job Location'] = pd.Series(job_location)\n",
    "    job_lst['Full_Job_Description'] = pd.Series(Full_Job_Description)\n",
    "    #job_lst = job_lst[:10] #As per requirment we need only first 10 records from the result\n",
    "\n",
    "    #Exporting Data from data frame to CSV file and making ZIP folder\n",
    "    compression_opts = dict(method='zip', archive_name='Job_report_of_Data_Scientist_Bangalore.csv')\n",
    "    return job_lst.to_csv('Job_report_of_Data_Scientist_Bangalore.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_DataScientist_Bangalore(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. In this question you have to scrape data using the filters available on the webpage as shown below:  You have to use the location and salary filter. You have to scrape data for “Data Scientist” designation for first 10 job results. You have to scrape the job-title, job-location, company_name, experience_required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_DataScientist_by_filter(abc):\n",
    "    import selenium\n",
    "    from selenium import webdriver\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    from selenium.common.exceptions import NoSuchElementException\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.common.by import By\n",
    "\n",
    "#1. first get the webpage https://www.naukri.com/\n",
    "\n",
    "    driver = webdriver.Chrome(r'C:\\Users\\udayakiran\\Desktop\\chromedriver.exe') # system local Chromedriver location path\n",
    "    driver.get(abc) #accessing the website using driver\n",
    "    time.sleep(3)\n",
    "\n",
    "#2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "\n",
    "            # Select the “Skill,Designations,Companies” field\n",
    "    Kw_box = driver.find_element_by_name('keyword')\n",
    "    Kw_box.send_keys(\"Data Scientist\") #Entering data into Search key skills box\n",
    "\n",
    "#3. Then click the search button.\n",
    "\n",
    "            # Find and click search button\n",
    "    search_button = driver.find_element_by_class_name('btn')\n",
    "    search_button.click()\n",
    "    time.sleep(5)\n",
    "    delay=20\n",
    "\n",
    "\n",
    "#4  Applying the location filter and salary filter by checking the respective boxes\n",
    "    #WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.LINK_TEXT, \"3-6 Lakhs\")))\n",
    "    sal_check_box = driver.find_element_by_xpath(\"//span[@title='3-6 Lakhs']\")\n",
    "    sal_check_box.click()\n",
    "    \n",
    "    time.sleep(5)\n",
    "    #WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.LINK_TEXT, \"Delhi/NCR\")))\n",
    "    loc_check_box = driver.find_element_by_xpath(\"//span[@title='Delhi/NCR']\")\n",
    "    loc_check_box.click()\n",
    "\n",
    "    Job_title,Company_Name,job_location,experience_required = [],[],[],[]\n",
    "    time.sleep(3)\n",
    "    #WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CLASS_NAME, \"title fw500 ellipsis\")))\n",
    "    for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")[:10]:\n",
    "        Job_title.append(i.text)\n",
    "    for i in driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")[:10]:\n",
    "        Company_Name.append(i.text)\n",
    "    for i in driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")[:10]:\n",
    "        job_location.append(i.text)\n",
    "    for i in driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")[:10]:\n",
    "        experience_required.append(i.text)\n",
    "    driver.close()\n",
    "\n",
    "#5. Finally create a dataframe of the scraped data\n",
    "\n",
    "    #Creating Data Frame for Job details storing\n",
    "    job_lst = pd.DataFrame({})\n",
    "    job_lst['Job Title'] = pd.Series(Job_title)\n",
    "    job_lst['Company Name'] = pd.Series(Company_Name)\n",
    "    job_lst['Job Location'] = pd.Series(job_location)\n",
    "    job_lst['Experience_Required'] = pd.Series(experience_required)\n",
    "    #job_lst = job_lst[:10] #As per requirment we need only first 10 records from the result\n",
    "\n",
    "    #Exporting Data from data frame to CSV file and making ZIP folder\n",
    "    compression_opts = dict(method='zip', archive_name='Job_report_of_Data_Scientist_delhi_3_6LPA.csv')\n",
    "    return job_lst.to_csv('Job_report_of_Data_Scientist_delhi_3_6LPA.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_DataScientist_by_filter(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter email to login Glassdoor website : \n",
      "uddhaykiran@gmail.com\n",
      "Please enter password to login Glassdoor website : \n",
      "thebavuki.9\n"
     ]
    }
   ],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "\n",
    "driver = webdriver.Chrome(r'C:\\Users\\udayakiran\\Desktop\\chromedriver.exe') # system local Chromedriver location path\n",
    "driver.get(\"https://www.glassdoor.co.in/\") #accessing the website using driver\n",
    "time.sleep(3)\n",
    "#2. we should login to proceed further as per question\n",
    "driver.find_element_by_xpath(\"//div[@class='locked-home-sign-in']\").click() #signin click\n",
    "print(\"Please enter email to login Glassdoor website : \")\n",
    "email_box = driver.find_element_by_id('userEmail').send_keys(input()) #username entry\n",
    "print(\"Please enter password to login Glassdoor website : \")\n",
    "password_box = driver.find_element_by_id('userPassword').send_keys(input()) #password entry\n",
    "driver.find_element_by_xpath(\"//button[@class='gd-ui-button minWidthBtn css-8i7bc2']\").click() # signin click\n",
    "time.sleep(5)\n",
    "#3. Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida” in “location” field.\n",
    "Kw_box = driver.find_element_by_name('sc.keyword').send_keys(\"Data Scientist\")# keyword text box\n",
    "driver.find_element_by_id(\"sc.location\").click()\n",
    "loc_box = driver.find_element_by_id('sc.location').clear()\n",
    "loc_box = driver.find_element_by_id('sc.location').send_keys(\"Noida\") #location text box\n",
    "driver.find_element_by_xpath(\"//button[@class='gd-ui-button ml-std col-auto SearchStyles__newSearchButton css-iixdfr']\").click() #search button\n",
    "time.sleep(5)\n",
    "#4. Then scrape the data for the first 10 jobs results you get in the above shown page.\n",
    "No_Of_Days_Ago_Posted=[i.text for i in driver.find_elements_by_xpath(\"//div[@class='d-flex align-items-end pl-std css-mi55ob']\")[:10]]\n",
    "Rating_Of_Company=[i.text for i in driver.find_elements_by_xpath(\"//div[@class='d-flex flex-column css-x75kgh e1rrn5ka3']/span[1]\")[:10]]\n",
    "Company_Name=[i.text for i in driver.find_elements_by_xpath(\"//div[@class='d-flex justify-content-between align-items-start']/a[1]/span[1]\")[:10]]\n",
    "#Store the data in a dataframe\n",
    "job_lst = pd.DataFrame({'Company Name' : pd.Series(Company_Name), 'Rating_Of_Company': pd.Series(Rating_Of_Company), 'No_Of_Days_Ago_Job_Posted':pd.Series(No_Of_Days_Ago_Posted)})\n",
    "\n",
    "#Exporting Data from data frame to CSV file and making ZIP folder\n",
    "compression_opts = dict(method='zip', archive_name='Job_report_of_glass_door.csv')\n",
    "job_lst.to_csv('Job_report_of_glass_door.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Write a python program to scrape the salary data for Data Scientist designation in Noida location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "\n",
    "driver = webdriver.Chrome(r'C:\\Users\\udayakiran\\Desktop\\chromedriver.exe') # system local Chromedriver location path\n",
    "driver.get(\"https://www.glassdoor.co.in/Salaries/index.htm\") #accessing the website using driver\n",
    "time.sleep(3)\n",
    "#2. Enter “Data Scientist” in Job title field and “Noida” in location field\n",
    "Kw_box = driver.find_element_by_name('sc.keyword').send_keys(\"Data Scientist\")# keyword text box\n",
    "driver.find_element_by_id(\"LocationSearch\").click()\n",
    "loc_box = driver.find_element_by_id('LocationSearch').clear()\n",
    "loc_box = driver.find_element_by_id('LocationSearch').send_keys(\"Noida\") #location text box\n",
    "#3. Click the search button.\n",
    "driver.find_element_by_xpath(\"//button[@class='gd-btn-mkt']\").click() #search button\n",
    "time.sleep(5)\n",
    "#4. Scrape data for first 10 companies. Scrape the min salary, max salary, company name, Average salary of the company.\n",
    "min_salary = [i.text for i in driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container ']/span[1]\")[:10]]\n",
    "max_salary = [i.text for i in driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container ']/span[2]\")[:10]]\n",
    "Average_salary = [i.text.replace('\\n','') for i in driver.find_elements_by_xpath(\"//div[@class='col-2 d-none d-md-flex flex-row justify-content-end']\")[:10]]\n",
    "company_name = [i.text for i in driver.find_elements_by_xpath(\"//div[@class='d-flex']/div[2]/p[2]\")[:10]]\n",
    "#Store the data in a dataframe\n",
    "job_lst = pd.DataFrame({'Company Name' : pd.Series(company_name), 'Average_salary': pd.Series(Average_salary), 'max_salary':pd.Series(max_salary), 'min_salary':pd.Series(min_salary)})\n",
    "#Exporting Data from data frame to CSV file and making ZIP folder\n",
    "compression_opts = dict(method='zip', archive_name='Salary_report_of_glass_door.csv')\n",
    "job_lst.to_csv('Salary_report_of_glass_door.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
